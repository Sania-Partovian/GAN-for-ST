# -*- coding: utf-8 -*-
"""LOG_GEN_GRU_GAN_FINAL_Zookeeper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12euGpPe_IDE9nYWZ4JzTh-RWhIOvRZMh
"""

import pandas as pd

# Load the dataset
data_path = '/content/Zookeeper_2k.log_structured.csv'
log_data = pd.read_csv(data_path)

# Display the first few rows of the dataset to understand its structure
log_data.head()

import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split

# Load and preprocess the data
data_path = '/content/Zookeeper_2k.log_structured.csv'
log_data = pd.read_csv(data_path)
log_data['DateTime'] = pd.to_datetime(log_data['Date'] + ' ' + log_data['Time'], format='%Y-%m-%d %H:%M:%S,%f')
log_data.drop(['Date', 'Time'], axis=1, inplace=True)
log_data['DateTime'] = log_data['DateTime'].values.astype(float) / 10**9  # Scale to make the values more manageable

# Create a dictionary to store each encoder
encoders = {}

# Encode categorical data including 'EventId' which contains strings
for col in ['Level', 'Node', 'Component', 'Content', 'EventTemplate', 'EventId']:
    encoder = LabelEncoder()
    log_data[col] = encoder.fit_transform(log_data[col])
    encoders[col] = encoder  # Store the encoder

# Normalize all columns to range [0, 1]
scaler = MinMaxScaler()
log_data = pd.DataFrame(scaler.fit_transform(log_data), columns=log_data.columns)

# Splitting the dataset
train_data, test_data = train_test_split(log_data, test_size=0.2, random_state=42)

print("Preprocessed data sample:")
print(train_data.head())

log_data.to_csv('log_data_SeqGAN.csv', index=False)

import tensorflow as tf

def make_generator_model(input_dim, output_dim):
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(input_dim, 1)),
        tf.keras.layers.GRU(256, return_sequences=True),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.LeakyReLU(),
        tf.keras.layers.Dropout(0.3),

        tf.keras.layers.GRU(128, return_sequences=True),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.LeakyReLU(),
        tf.keras.layers.Dropout(0.3),

        tf.keras.layers.GRU(64, return_sequences=True),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.LeakyReLU(),
        tf.keras.layers.Dropout(0.3),

        tf.keras.layers.GRU(32),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.LeakyReLU(),
        tf.keras.layers.Dropout(0.3),

        tf.keras.layers.Dense(output_dim, activation='tanh')
    ])
    return model

def make_discriminator_model(input_dim):
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(input_dim, 1)),
        tf.keras.layers.GRU(256, return_sequences=True),
        tf.keras.layers.LeakyReLU(),
        tf.keras.layers.Dropout(0.3),

        tf.keras.layers.GRU(128, return_sequences=True),
        tf.keras.layers.LeakyReLU(),
        tf.keras.layers.Dropout(0.3),

        tf.keras.layers.GRU(64, return_sequences=True),
        tf.keras.layers.LeakyReLU(),
        tf.keras.layers.Dropout(0.3),

        tf.keras.layers.GRU(32),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.LeakyReLU(),
        tf.keras.layers.Dropout(0.3),

        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    return model

# Define input and output dimensions
input_dim = 100  # Dimension of the noise input to the generator
output_dim = train_data.shape[1]

# Instantiate the models
generator = make_generator_model(input_dim, output_dim)
discriminator = make_discriminator_model(output_dim)

# Compile the discriminator
discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the GAN model
discriminator.trainable = False  # Freeze the discriminator during generator training
gan_input = tf.keras.layers.Input(shape=(input_dim, 1))
gan_output = discriminator(generator(gan_input))
gan = tf.keras.models.Model(gan_input, gan_output)
gan.compile(optimizer='adam', loss='binary_crossentropy')

import numpy as np

import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score

def train_gan(generator, discriminator, gan, data, epochs=100, batch_size=32, input_dim=100):
    for epoch in range(epochs):
        # Random noise for generator
        noise = np.random.normal(0, 1, size=(batch_size, input_dim))
        # Generate fake data
        generated_data = generator.predict(noise)
        # Get a random set of real data samples
        idx = np.random.randint(0, data.shape[0], batch_size)
        real_data = data[idx]  # Use numpy indexing directly
        # Labels for training
        real_labels = np.ones((batch_size, 1))
        fake_labels = np.zeros((batch_size, 1))
        # Train discriminator
        d_loss_real = discriminator.train_on_batch(real_data, real_labels)
        d_loss_fake = discriminator.train_on_batch(generated_data, fake_labels)
        # Train generator
        g_loss = gan.train_on_batch(noise, real_labels)

        # Calculate F1 score
        # Get discriminator predictions
        real_preds = discriminator.predict(real_data)
        fake_preds = discriminator.predict(generated_data)
        # Convert predictions to binary labels
        real_preds = (real_preds > 0.5).astype(int)
        fake_preds = (fake_preds > 0.5).astype(int)
        # Concatenate labels and predictions
        all_labels = np.concatenate([real_labels, fake_labels])
        all_preds = np.concatenate([real_preds, fake_preds])
        # Calculate precision, recall, and F1 score
        precision = precision_score(all_labels, all_preds)
        recall = recall_score(all_labels, all_preds)
        f1 = f1_score(all_labels, all_preds)

        # Logging for visibility
        if (epoch + 1) % 10 == 0:
            print(f'Epoch {epoch + 1}, Discriminator Loss Real: {d_loss_real}, Discriminator Loss Fake: {d_loss_fake}, Generator Loss: {g_loss}, F1 Score: {f1}, Precision: {precision}, Recall: {recall}')

# Convert entire DataFrame to numpy array for training
train_data_np = train_data.values
# Prepare to train
train_gan(generator, discriminator, gan, train_data_np, epochs=1000)


# # Convert entire DataFrame to numpy array for training
# train_data_np = train_data.values
# # Prepare to train
# train_gan(generator, discriminator, gan, train_data_np, epochs=5000)

def generate_synthetic_data(generator, num_samples, input_dim):
    noise = np.random.normal(0, 1, size=(num_samples, input_dim))
    synthetic_data = generator.predict(noise)
    return synthetic_data

# Generate synthetic log data
num_samples = 1000  # Specify the number of synthetic samples you want to generate
input_dim = 100  # Dimension of the noise input to the generator
synthetic_logs = generate_synthetic_data(generator, num_samples, input_dim)

synthetic_logs_df = pd.DataFrame(synthetic_logs, columns=log_data.columns)

synthetic_logs_df.to_csv('synthetic_logs_SeqGAN.csv', index=False)

def decode_synthetic_data(synthetic_data, encoders, columns):
    synthetic_df = pd.DataFrame(synthetic_data.squeeze(), columns=columns)
    for col, encoder in encoders.items():
        # Get the range of valid labels
        min_label, max_label = 0, len(encoder.classes_) - 1
        # Clip values to ensure they fall within the valid range
        clipped_values = np.clip(synthetic_df[col].round().astype(int), min_label, max_label)
        synthetic_df[col] = encoder.inverse_transform(clipped_values)
    return synthetic_df

# Example usage
synthetic_logs_df = decode_synthetic_data(synthetic_logs, encoders, train_data.columns)

print("Sample Generated Log Content:")
synthetic_logs_df

org_data = pd.read_csv(data_path)
common_columns = synthetic_logs_df.columns.intersection(org_data.columns)
org_data_filtered = org_data[common_columns]
org_data_filtered

!pip install table_evaluator
from table_evaluator import load_data, TableEvaluator

org_data_filtered.head()

synthetic_logs_df = synthetic_logs_df.drop(columns=['DateTime'])
# synthetic_logs_df.head()

# table_evaluator =  TableEvaluator(org_data_filtered, synthetic_logs_df)

# table_evaluator.visual_evaluation()

# import matplotlib.pyplot as plt

# # Access the generated figures and modify the titles
# figures = [plt.figure(n) for n in plt.get_fignums()]
# for fig in figures:
#     for ax in fig.get_axes():
#         # Customize the title
#         if "Fake data" in ax.get_title():
#             ax.set_title(ax.get_title().replace("Fake data", "Synthetic data"))
#         elif "Real data" in ax.get_title():
#             ax.set_title(ax.get_title().replace("Real data", "Original data"))

# # Optionally, save or display the modified figures
# for fig in figures:
#     fig.savefig(f'/content/custom_{fig.get_label()}.png')
#     plt.show()

log_data.head(2)

synthetic_logs_unprocessed = pd.read_csv("/content/synthetic_logs_SeqGAN.csv")
synthetic_logs_unprocessed.head(2)

table_evaluator =  TableEvaluator(log_data, synthetic_logs_unprocessed)
table_evaluator.visual_evaluation()

