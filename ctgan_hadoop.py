# -*- coding: utf-8 -*-
"""CTGAN_Hadoop.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bMEUzhL2hjoj5NlUt0q5xh3fjMKELYME
"""

import pandas as pd

file_path = "/content/Hadoop_2k.log_structured.csv"

df = pd.read_csv(file_path)

print(df)

# Assign a unique number to each value

df['Time_id'], categories = pd.factorize(df['Time'])

df['Level_Id'], categories = pd.factorize(df['Level'])

df['Process_Id'], categories = pd.factorize(df['Process'])

df['Component_Id'], categories = pd.factorize(df['Component'])

df['Content_Id'], categories = pd.factorize(df['Content'])

df['EventID_Id'], categories = pd.factorize(df['EventId'])

df['EventTemplate_Id'], categories = pd.factorize(df['EventTemplate'])



# Display the DataFrame
print(df)

df.to_csv('Preprocessed_data.csv', index=False)

pip install ctgan

from ctgan import CTGAN

new_df = df[['Time_id', 'Level_Id', 'Process_Id','Component_Id', 'Content_Id', 'EventID_Id','EventTemplate_Id' ]].copy()

new_df = new_df.dropna().reset_index(drop=True)

print(new_df)

ctgan = CTGAN()

ctgan.fit(new_df, epochs=3000)

# Generate synthetic log data
synthetic_data = ctgan.sample(1000)

# Convert the synthetic log data to a DataFrame
synthetic_log_df = pd.DataFrame(synthetic_data, columns=new_df.columns)

mapping_df = pd.read_csv('Preprocessed_data.csv')


# Set the ID column as the index for easy mapping
mapping_df.set_index('Time_id', inplace=True)

# Create a mapping dictionary from ID to name
id_to_name = mapping_df['Time'].to_dict()

# Replace the IDs with corresponding names in the synthetic log data
synthetic_log_df['Time_id'] = synthetic_log_df['Time_id'].map(id_to_name)

# Set the ID column as the index for easy mapping
mapping_df.set_index('Level_Id', inplace=True)

# Create a mapping dictionary from ID to name
id_to_name = mapping_df['Level'].to_dict()

# Replace the IDs with corresponding names in the synthetic log data
synthetic_log_df['Level_Id'] = synthetic_log_df['Level_Id'].map(id_to_name)

# Set the ID column as the index for easy mapping
mapping_df.set_index('Process_Id', inplace=True)

# Create a mapping dictionary from ID to name
id_to_name = mapping_df['Process'].to_dict()

# Replace the IDs with corresponding names in the synthetic log data
synthetic_log_df['Process_Id'] = synthetic_log_df['Process_Id'].map(id_to_name)

# Set the ID column as the index for easy mapping
mapping_df.set_index('Component_Id', inplace=True)

# Create a mapping dictionary from ID to name
id_to_name = mapping_df['Component'].to_dict()

# Replace the IDs with corresponding names in the synthetic log data
synthetic_log_df['Component_Id'] = synthetic_log_df['Component_Id'].map(id_to_name)

# Set the ID column as the index for easy mapping
mapping_df.set_index('Content_Id', inplace=True)

# Create a mapping dictionary from ID to name
id_to_name = mapping_df['Content'].to_dict()

# Replace the IDs with corresponding names in the synthetic log data
synthetic_log_df['Content_Id'] = synthetic_log_df['Content_Id'].map(id_to_name)

# Set the ID column as the index for easy mapping
mapping_df.set_index('EventID_Id', inplace=True)

# Create a mapping dictionary from ID to name
id_to_name = mapping_df['EventId'].to_dict()

# Replace the IDs with corresponding names in the synthetic log data
synthetic_log_df['EventID_Id'] = synthetic_log_df['EventID_Id'].map(id_to_name)

# Set the ID column as the index for easy mapping
mapping_df.set_index('EventTemplate_Id', inplace=True)

# Create a mapping dictionary from ID to name
id_to_name = mapping_df['EventTemplate'].to_dict()

# Replace the IDs with corresponding names in the synthetic log data
synthetic_log_df['EventTemplate_Id'] = synthetic_log_df['EventTemplate_Id'].map(id_to_name)

print(synthetic_log_df)

synthetic_log_df.to_csv('synthetic_data_final.csv', index=False)

pip install table-evaluator

from table_evaluator import load_data, TableEvaluator

synthetic_df =  pd.read_csv("/content/synthetic_data.csv")


Preprocessed_data = pd.read_csv("/content/Preprocessed_data.csv")

synthetic_df

Preprocessed_data

columns_to_drop=['LineId','Date','Time', 'Level','Process','Component','Content','EventId','EventTemplate','Date_Id']

Preprocessed_data = Preprocessed_data.drop(columns = columns_to_drop )

Preprocessed_data

categorical_features = ['Time_id','Level_Id','Process_Id','Component_Id','Content_Id','EventID_Id','EventTemplate_Id']

table_evaluator =  TableEvaluator(Preprocessed_data, synthetic_df)

table_evaluator.visual_evaluation()