# -*- coding: utf-8 -*-
"""LOG-GEN-GANs-zookeeper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CgB6sdwUfIrr1EFJk6G_Cgi7bEn1kU2V
"""

import pandas as pd

# Load the dataset
data_path = '/content/Zookeeper_2k.log_structured.csv'
log_data = pd.read_csv(data_path)

# Display the first few rows of the dataset to understand its structure
log_data.head()

import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split

# Load and preprocess the data
data_path = '/content/Zookeeper_2k.log_structured.csv'
log_data = pd.read_csv(data_path)
log_data['DateTime'] = pd.to_datetime(log_data['Date'] + ' ' + log_data['Time'], format='%Y-%m-%d %H:%M:%S,%f')
log_data.drop(['Date', 'Time'], axis=1, inplace=True)
log_data['DateTime'] = log_data['DateTime'].values.astype(float) / 10**9  # Scale to make the values more manageable

# Create a dictionary to store each encoder
encoders = {}

# Encode categorical data including 'EventId' which contains strings
for col in ['Level', 'Node', 'Component', 'Content', 'EventTemplate', 'EventId']:
    encoder = LabelEncoder()
    log_data[col] = encoder.fit_transform(log_data[col])
    encoders[col] = encoder  # Store the encoder

# Normalize all columns to range [0, 1]
scaler = MinMaxScaler()
log_data = pd.DataFrame(scaler.fit_transform(log_data), columns=log_data.columns)

# Splitting the dataset
train_data, test_data = train_test_split(log_data, test_size=0.2, random_state=42)

print("Preprocessed data sample:")
print(train_data.head())

def make_generator_model(input_dim, output_dim):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(128, input_dim=input_dim, activation='relu'))
    model.add(tf.keras.layers.Dense(64, input_dim=128, activation='relu'))
    model.add(tf.keras.layers.Dense(32, input_dim=64, activation='relu'))
    model.add(tf.keras.layers.Dense(output_dim, activation='tanh'))
    return model

def make_discriminator_model(input_dim):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(128, input_dim=input_dim, activation='relu'))
    model.add(tf.keras.layers.Dense(64, input_dim=128, activation='relu'))
    model.add(tf.keras.layers.Dense(32, input_dim=64, activation='relu'))
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
    return model

# Constants and model dimensions
input_dim = 100  # Dimension of random noise vector.
output_dim = train_data.shape[1]  # Number of features in the dataset

generator = make_generator_model(input_dim, output_dim)
discriminator = make_discriminator_model(output_dim)

# Compile discriminator
discriminator.compile(loss='binary_crossentropy', optimizer='adam')
# Combined model (stacked generator and discriminator)
# The generator tries to fool the discriminator, hence we train only the generator in this setup
discriminator.trainable = False
gan_input = tf.keras.Input(shape=(input_dim,))
fake_output = generator(gan_input)
gan_output = discriminator(fake_output)
gan = tf.keras.Model(gan_input, gan_output)
gan.compile(loss='binary_crossentropy', optimizer='adam')

import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score

def train_gan(generator, discriminator, gan, data, epochs=100, batch_size=32, input_dim=100):
    for epoch in range(epochs):
        # Random noise for generator
        noise = np.random.normal(0, 1, size=(batch_size, input_dim))
        # Generate fake data
        generated_data = generator.predict(noise)
        # Get a random set of real data samples
        idx = np.random.randint(0, data.shape[0], batch_size)
        real_data = data[idx]  # Use numpy indexing directly
        # Labels for training
        real_labels = np.ones((batch_size, 1))
        fake_labels = np.zeros((batch_size, 1))
        # Train discriminator
        d_loss_real = discriminator.train_on_batch(real_data, real_labels)
        d_loss_fake = discriminator.train_on_batch(generated_data, fake_labels)
        # Train generator
        g_loss = gan.train_on_batch(noise, real_labels)

        # Calculate F1 score
        # Get discriminator predictions
        real_preds = discriminator.predict(real_data)
        fake_preds = discriminator.predict(generated_data)
        # Convert predictions to binary labels
        real_preds = (real_preds > 0.5).astype(int)
        fake_preds = (fake_preds > 0.5).astype(int)
        # Concatenate labels and predictions
        all_labels = np.concatenate([real_labels, fake_labels])
        all_preds = np.concatenate([real_preds, fake_preds])
        # Calculate precision, recall, and F1 score
        precision = precision_score(all_labels, all_preds)
        recall = recall_score(all_labels, all_preds)
        f1 = f1_score(all_labels, all_preds)

        # Logging for visibility
        if (epoch + 1) % 10 == 0:
            print(f'Epoch {epoch + 1}, Discriminator Loss Real: {d_loss_real}, Discriminator Loss Fake: {d_loss_fake}, Generator Loss: {g_loss}, F1 Score: {f1}, Precision: {precision}, Recall: {recall}')

# Convert entire DataFrame to numpy array for training
train_data_np = train_data.values
# Prepare to train
train_gan(generator, discriminator, gan, train_data_np, epochs=1000)

def generate_synthetic_data(generator, num_samples, input_dim):
    noise = np.random.normal(0, 1, size=(num_samples, input_dim))
    synthetic_data = generator.predict(noise)
    return synthetic_data

# Generate synthetic log data
num_samples = 1000  # Specify the number of synthetic samples you want to generate
input_dim = 100  # Dimension of the noise input to the generator
synthetic_logs = generate_synthetic_data(generator, num_samples, input_dim)

def decode_synthetic_data(synthetic_data, encoders, columns):
    synthetic_df = pd.DataFrame(synthetic_data, columns=columns)
    for col, encoder in encoders.items():
        # Get the range of valid labels
        min_label, max_label = 0, len(encoder.classes_) - 1
        # Clip values to ensure they fall within the valid range
        clipped_values = np.clip(synthetic_df[col].round().astype(int), min_label, max_label)
        synthetic_df[col] = encoder.inverse_transform(clipped_values)
    return synthetic_df

# Example usage
synthetic_logs_df = decode_synthetic_data(synthetic_logs, encoders, train_data.columns)

print("Sample Generated Log Content:")
synthetic_logs_df
# synthetic_logs_df = synthetic_logs_df.drop(['DateTime'], axis=1, inplace=True)

org_data = pd.read_csv(data_path)
common_columns = synthetic_logs_df.columns.intersection(org_data.columns)
org_data_filtered = org_data[common_columns]
org_data_filtered

!pip install table_evaluator
from table_evaluator import load_data, TableEvaluator

log_data.head(2)

synthetic_logs_unprocessed_ = pd.DataFrame(synthetic_logs, columns=log_data.columns)
synthetic_logs_unprocessed_.head(2)

table_evaluator =  TableEvaluator(log_data, synthetic_logs_unprocessed_)

table_evaluator.visual_evaluation()

"""### **NOW WE ARE GONNA USE BERT FOR FEATURE EXTRACTION**"""